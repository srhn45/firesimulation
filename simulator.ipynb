{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4252f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from math import exp, sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9545a3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from scipy.sparse import lil_matrix\n",
    "import os\n",
    "import math\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "#from functions import gaussian_mixture_field, make_heatmap_gif, adjacency_matrix, block_diag_batch\n",
    "#from classes import Simulator, FireGraph, BelieverModel, DatasetGenerator, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ce4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture_field(size, n_components=None):\n",
    "    '''\n",
    "    Generate random topology (height distribution).\n",
    "    '''\n",
    "    x, y = np.meshgrid(np.linspace(0, 1, size), np.linspace(0, 1, size))\n",
    "    field = np.zeros((size, size))\n",
    "\n",
    "    if not n_components:\n",
    "        n_components = np.random.poisson(20)\n",
    "\n",
    "    for _ in range(n_components):\n",
    "        cx, cy = np.random.uniform(0, 1, 2) # random center\n",
    "        sx, sy = np.random.uniform(0.01, 0.2, 2) # random covariance scale\n",
    "        w = np.random.exponential(10) # weight scale\n",
    "\n",
    "        gaussian = w * np.exp(-(((x - cx) ** 2) / (2 * sx**2) +\n",
    "                                ((y - cy) ** 2) / (2 * sy**2)))\n",
    "        field += gaussian\n",
    "\n",
    "    field = (1 - (field - field.min()) / (field.max() - field.min()))*2\n",
    "    return field\n",
    "\n",
    "def make_heatmap_gif(simulator, filename=\"simulation.gif\", cmap=\"plasma\"):\n",
    "    '''\n",
    "    Create gif of simulation results.\n",
    "    '''\n",
    "    \n",
    "    times = sorted(simulator.maps.keys())\n",
    "    frames = [simulator.maps[t] for t in times]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # --- fire as background ---\n",
    "    vmin, vmax = np.min(frames), np.max(frames)\n",
    "    fire_img = ax.imshow(frames[0], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # --- terrain overlay ---\n",
    "    terrain_img = ax.imshow(simulator.terrain, cmap=\"Greens\", alpha=0.2)  # low alpha on top\n",
    "\n",
    "    cbar = fig.colorbar(fire_img, ax=ax)\n",
    "    cbar.set_label(\"Fire Intensity\", rotation=270, labelpad=15)\n",
    "\n",
    "    def update(frame):\n",
    "        fire_img.set_data(frame)    \n",
    "        return [fire_img, terrain_img]\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, update, frames=frames, interval=60, blit=True\n",
    "    )\n",
    "\n",
    "    ani.save(filename, writer=\"pillow\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def adjacency_matrix(length, width):\n",
    "    '''\n",
    "    Sparse adjacency matrix generator for an n x n undirected graph, where each node is connected to its immediate neighbors and itself.\n",
    "    '''\n",
    "    N = length * width\n",
    "    adj = lil_matrix((N, N), dtype=np.float32)\n",
    "    directions = [(-1,0), (-1,1), (0,1), (1,1), (1,0), (1,-1), (0,-1), (-1,-1), (0,0)] # 8 sided\n",
    "    for i in range(N):\n",
    "        x, y = divmod(i, width)\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x+dx, y+dy\n",
    "            if 0 <= nx < length and 0 <= ny < width:\n",
    "                j = nx*width + ny\n",
    "                adj[i,j] = 1\n",
    "    return adj.tocoo()\n",
    "\n",
    "def block_diag_batch(adj, batch_size):\n",
    "    \"\"\"\n",
    "    Build block-diagonal adjacency matrix for efficient processing of batches.\n",
    "\n",
    "    This function now calls `.coalesce()` on the incoming sparse tensor so it's safe to access\n",
    "    `.indices()` and `.values()` without raising \"Cannot get indices on an uncoalesced tensor\".\n",
    "    \"\"\"\n",
    "    # Ensure the sparse tensor is coalesced before reading indices/values\n",
    "    adj = adj.coalesce()\n",
    "\n",
    "    N = adj.size(0)\n",
    "    indices = adj.indices()  # shape (2, E)\n",
    "    values = adj.values()\n",
    "\n",
    "    device = indices.device\n",
    "\n",
    "    offsets = torch.arange(batch_size, device=device) * N\n",
    "    offsets = offsets.view(1, -1, 1)\n",
    "\n",
    "    expanded = indices.unsqueeze(1).expand(-1, batch_size, -1)\n",
    "    expanded = expanded + offsets\n",
    "    expanded = expanded.permute(1, 0, 2).reshape(2, -1)\n",
    "\n",
    "    expanded_values = values.repeat(batch_size)\n",
    "\n",
    "    size = (N * batch_size, N * batch_size)\n",
    "    # Return a coalesced sparse tensor for downstream operations\n",
    "    return torch.sparse_coo_tensor(expanded, expanded_values, size=size, device=device).coalesce()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e39f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator():\n",
    "    def __init__(self, size=256, wind_speed=0, wind_direction=[0,0], response_rate=0.1, response_start=20, base_spread_rate=0.3, n_components=None, decay_rate=1e-3):\n",
    "        self.size = size\n",
    "        self.map = np.zeros((size, size))\n",
    "        self.wind_speed = wind_speed\n",
    "        self.wind_direction = wind_direction\n",
    "        self.response_rate = response_rate\n",
    "        self.response_start = response_start\n",
    "        self.spread_rate = base_spread_rate\n",
    "        self.time = 0\n",
    "        self.decay_rate = decay_rate\n",
    "        self.maps = {}\n",
    "\n",
    "        self.terrain = gaussian_mixture_field(size, n_components=n_components)\n",
    "\n",
    "    def step(self):\n",
    "        new_map = deepcopy(self.map)\n",
    "\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if self.map[i, j] >= 1:\n",
    "                    if np.random.rand() < self.spread_rate*self.terrain[i, j]*np.exp(-self.decay_rate * self.time) and new_map[i, j] < 5:\n",
    "                        new_map[i, j] += 1\n",
    "                    \n",
    "                    for di in [-1, 0, 1]:\n",
    "                        for dj in [-1, 0, 1]:\n",
    "                            if di == 0 and dj == 0:\n",
    "                                continue\n",
    "\n",
    "                            ni, nj = i + di, j + dj\n",
    "                            spread_chance = self.spread_rate*self.map[i,j]\n",
    "\n",
    "                            if 0 <= ni < self.size and 0 <= nj < self.size:\n",
    "                                if self.wind_speed > 0:\n",
    "                                    wind_influence = (di * self.wind_direction[0] + dj * self.wind_direction[1]) / (np.linalg.norm(self.wind_direction) + 1e-6)\n",
    "                                    wind_influence *= np.random.normal(1, 0.5)\n",
    "\n",
    "\n",
    "                                    if wind_influence > 0:\n",
    "                                        spread_chance *= (1 + self.wind_speed * wind_influence)\n",
    "                                    spread_chance *= self.terrain[ni, nj]\n",
    "                                    spread_chance *= np.exp(-self.decay_rate * self.time)\n",
    "                                    spread_chance = np.clip(spread_chance, 0, 1)\n",
    "\n",
    "                                if np.random.rand() < spread_chance and new_map[ni, nj] <= new_map[i, j]:\n",
    "                                    if new_map[ni, nj] < 5:\n",
    "                                        if np.random.rand() <= exp(-self.time/1000):\n",
    "                                            new_map[ni, nj] += 1\n",
    "\n",
    "                                if self.time >= self.response_start and new_map[ni, nj] == 0:\n",
    "                                    if np.random.rand() < 1 - exp(-(self.response_rate*(0.5+self.terrain[i,j]) * (self.time - self.response_start))):\n",
    "                                        if new_map[i, j] > 0:\n",
    "                                            new_map[i, j] -= 1 # Firefighting effort\n",
    "                            if np.exp(-self.decay_rate * self.time) < 0.5:\n",
    "                                if ni < 0 or ni >= self.size or nj < 0 or nj >= self.size:\n",
    "                                    if np.random.rand() < 1 - exp(-(self.response_rate) * (self.time - self.response_start)):\n",
    "                                        if new_map[i, j] > 0:\n",
    "                                            new_map[i, j] -= 1 # Edge effect\n",
    "                    \n",
    "                else:\n",
    "                    if 1 < i < self.size - 1 and 1 < j < self.size - 1:\n",
    "                        neighbors_on_fire = np.sum(self.map[i-1:i+2, j-1:j+2] >= 1) - (1 if self.map[i, j] >= 1 else 0)\n",
    "                        if neighbors_on_fire >= 6 and new_map[i, j] == 0:\n",
    "                            new_map[i, j] += 1\n",
    "        \n",
    "        self.maps[self.time] = deepcopy(self.map)\n",
    "        self.map = new_map\n",
    "        self.time += 1\n",
    "\n",
    "    def simulate(self):\n",
    "        nodes = max(np.random.poisson(3), 1)\n",
    "\n",
    "        x_init, y_init = np.random.randint(0, self.size, size=2)\n",
    "        self.map[x_init, y_init] = max(min(np.random.poisson(3), 5), 1)\n",
    "\n",
    "        for _ in range(nodes - 1):\n",
    "            while True:\n",
    "                x, y = np.random.randint(-20, 21, size=2)\n",
    "                new_x, new_y = x_init + x, y_init + y\n",
    "\n",
    "                if 0 <= new_x < self.size and 0 <= new_y < self.size:\n",
    "                    if self.map[new_x, new_y] == 0:\n",
    "                        self.map[new_x, new_y] = min(np.random.poisson(3), 5)\n",
    "                    break\n",
    "    \n",
    "        while np.any(self.map > 0):\n",
    "            self.step()\n",
    "\n",
    "class FireGraph(Dataset):\n",
    "    def __init__(self, length=256, width=256, path=\"simulation_data\"):\n",
    "        self.path = path\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "\n",
    "\n",
    "        adj = adjacency_matrix(self.length, self.width)\n",
    "        self.adjacency_matrix = torch.sparse_coo_tensor(\n",
    "            indices=torch.tensor(np.vstack((adj.row, adj.col)), dtype=torch.long),\n",
    "            values=torch.tensor(adj.data, dtype=torch.float32),\n",
    "            size=adj.shape\n",
    "        )\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "        self.save_dir = os.path.join(self.path, f\"{self.length}x{self.width}\")\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def generate_data(self, topology:np.array=None, past_info:np.array=None, wind_direction:np.array=np.array([0,0]),\n",
    "                         wind_speed:int=0, time:int=0, label:np.array=None):\n",
    "        '''\n",
    "        Generates training data from the simulation configurations and results.\n",
    "        '''\n",
    "\n",
    "        flat_topo = topology.ravel()\n",
    "        flat_info = past_info[:, :, 0].ravel()\n",
    "        flat_info_date = past_info[:, :, 1].ravel()\n",
    "        flat_label = label.ravel()\n",
    "        \n",
    "        data = np.stack([\n",
    "            flat_topo,\n",
    "            flat_info,\n",
    "            flat_info_date,\n",
    "            np.full(flat_topo.shape, wind_direction[0], dtype=np.float32),\n",
    "            np.full(flat_topo.shape, wind_direction[1], dtype=np.float32),\n",
    "            np.full(flat_topo.shape, wind_speed, dtype=np.float32),\n",
    "            np.full(flat_topo.shape, time, dtype=np.float32),\n",
    "            flat_label\n",
    "        ], axis=1)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def save_data(self, data:np.array):\n",
    "        '''\n",
    "        Saves data to drive.\n",
    "        '''\n",
    "        np.save(os.path.join(self.save_dir, f\"{time.time():.0f}.npy\"), data)\n",
    "    \n",
    "    def generate_dataset(self, pct=0.2):\n",
    "        '''\n",
    "        Reads data on drive to be used in training.\n",
    "        pct = percentage of data to be read from disk\n",
    "        '''\n",
    "        arrays = []\n",
    "        for file in os.listdir(self.save_dir):\n",
    "            if file.endswith(\".npy\"):\n",
    "                arr = np.load(os.path.join(self.save_dir, file), mmap_mode='r')\n",
    "                n = arr.shape[0]\n",
    "                m = math.ceil(pct * n)\n",
    "                idx = np.random.choice(n, size=m, replace=False)\n",
    "                arrays.append(arr[idx])\n",
    "\n",
    "        if arrays:\n",
    "            self.data = np.concatenate(arrays, axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        arr = self.data[idx]  # arr shape = (N, F+1)\n",
    "        x = arr[:, :-1]       # (N, F)\n",
    "        y = arr[:, -1]        # (N,)\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(y).long()\n",
    "    \n",
    "\n",
    "# referenced https://github.com/gordicaleksa/pytorch-GAT/blob/main/The%20Annotated%20GAT%20(Cora).ipynb for some of the code.\n",
    "\n",
    "class BelieverModel(nn.Module):\n",
    "    def __init__(self, nodes=256*256, input_features=7, num_layers=3, num_heads=3, num_features_per_head=4, num_output_classes=6, dropout=False):\n",
    "        super().__init__()\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = False\n",
    "        self.N = nodes\n",
    "        self.num_heads = num_heads\n",
    "        self.num_features_per_head = num_features_per_head\n",
    "\n",
    "        self.initial_transformation = nn.Linear(input_features, num_heads * num_features_per_head, bias=False)\n",
    "        nn.init.xavier_normal_(self.initial_transformation.weight)\n",
    "\n",
    "        self.a_lefts = nn.ParameterList()\n",
    "        self.a_rights = nn.ParameterList()\n",
    "        self.Ws = nn.ParameterList()\n",
    "        for i in range(num_layers):\n",
    "            a_left = nn.Parameter(torch.zeros(size=(num_heads, num_features_per_head)))\n",
    "            nn.init.xavier_uniform_(a_left)\n",
    "            a_right = nn.Parameter(torch.zeros(size=(num_heads, num_features_per_head)))\n",
    "            nn.init.xavier_uniform_(a_right)\n",
    "            W = nn.Parameter(torch.zeros(size=(num_heads, num_features_per_head, num_features_per_head)))\n",
    "            nn.init.xavier_normal_(W)\n",
    "\n",
    "            self.a_lefts.append(a_left)\n",
    "            self.a_rights.append(a_right)\n",
    "            self.Ws.append(W)\n",
    "        \n",
    "        self.final_transformation = nn.Linear(num_heads*num_features_per_head, num_output_classes)\n",
    "        nn.init.xavier_normal_(self.final_transformation.weight)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # adj = (N, N) adjacency matrix, in coo matrix format\n",
    "        # x = inputs (N, F_in)\n",
    "\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        N = x.size(0)\n",
    "        x = self.initial_transformation(x) # (N, F_out*H)\n",
    "        x = x.view(N, self.num_heads, self.num_features_per_head) # (N, H, F_out)\n",
    "\n",
    "        for W, a_left, a_right in zip(self.Ws, self.a_lefts, self.a_rights):\n",
    "            # W = (H, F_out, F_out)\n",
    "            # a_left = (H, F_out)\n",
    "            # a_right = (H, F_out)\n",
    "\n",
    "            # alpha_i,j = exp(a * [h_i||h_j] * adj[i,j]) / sum_j(exp(a * [h_i||h_j] * adj[i,j])), softmax\n",
    "            # h(i') = adj * sum_j (alpha_i,j * W * h_j)\n",
    "            # to simplify, we split a into 2 parts a_left and a_right, and calculate the attention scores for each of those parts, then sum up the scores only for viable pairs for computational efficiency\n",
    "\n",
    "            h_prime = torch.einsum(\"nhf,hfo->nho\", x, W) # (N, H, F_out) x (H, F_out, F_out) -> (N, H, F_out)\n",
    "\n",
    "            source_scores = (h_prime * a_left).sum(-1) # elementwise product, (N, H)\n",
    "            neighbor_scores = (h_prime * a_right).sum(-1) # (N, H)\n",
    "\n",
    "            adj = adj.coalesce()\n",
    "            row, col = adj.indices()\n",
    "            row = row.long(); col = col.long()\n",
    "            e = self.leakyrelu(source_scores[row] + neighbor_scores[col]) # (E, H) where E is the number of edges\n",
    "\n",
    "            H =e.size(1)\n",
    "            if hasattr(torch.Tensor, \"scatter_reduce\"):\n",
    "                max_per_node = torch.zeros((N, H), device=e.device, dtype=e.dtype).scatter_reduce(\n",
    "                    0, row.unsqueeze(-1).expand(-1,H), e, reduce=\"amax\", include_self=False\n",
    "                ) # maximum score among all neighbors of node i, (N, H)\n",
    "            else:\n",
    "                # fallback: compute max per node manually (safe but slower)\n",
    "                max_per_node = torch.full((N,H), -1e9, device=e.device, dtype=e.dtype)\n",
    "                for i_edge, i_node in enumerate(row):\n",
    "                    max_per_node[i_node] = torch.maximum(max_per_node[i_node], e[i_edge])\n",
    "            exp_e = torch.exp(e - max_per_node[row]) # for numerical stability, (E, H)\n",
    "\n",
    "            denom = torch.zeros((N, H), device=e.device, dtype=e.dtype)\n",
    "            denom.index_add_(0, row, exp_e) # summation over neighbors, (N, H)\n",
    "\n",
    "            alpha = exp_e / (denom[row] + 1e-9) # elementwise division, (E, H)\n",
    "\n",
    "            messages = h_prime[col] * alpha.unsqueeze(-1) # (E, H, F)\n",
    "\n",
    "            out = torch.zeros_like(h_prime, device=h_prime.device, dtype=h_prime.dtype)\n",
    "            out.index_add_(0, row, messages) # summation over neighbors again, (N, H, F)\n",
    "            x = self.relu(out)\n",
    "        \n",
    "        x = x.reshape(N, self.num_heads * self.num_features_per_head)\n",
    "        logits = self.final_transformation(x)\n",
    "        return logits\n",
    "\n",
    "class DatasetGenerator():\n",
    "    def __init__(self, size=128, wind_speed=1.5, wind_direction=[1,2], response_rate=0.03, response_start=100, base_spread_rate=0.03, perturb=True):\n",
    "        self.size = size\n",
    "        self.wind_speed = wind_speed\n",
    "        self.wind_direction = wind_direction\n",
    "        self.response_rate = response_rate\n",
    "        self.response_start = response_start\n",
    "        self.base_spread_rate = base_spread_rate\n",
    "        self.perturb = perturb\n",
    "        self.dataset = FireGraph(length=self.size, width=self.size)\n",
    "\n",
    "    def generate(self, num_sims=100, std_dev=0.2):\n",
    "        '''\n",
    "        Generates simulations and saves their data to drive. Samples data randomly for initial model training.\n",
    "        '''\n",
    "\n",
    "        for i in range(num_sims):\n",
    "            if self.perturb:\n",
    "                \n",
    "                simulator = Simulator(size=self.size, wind_speed=np.random.normal(self.wind_speed, std_dev*self.wind_speed), wind_direction=[np.random.uniform(-1, 1), np.random.uniform(-1,1)], \n",
    "                    response_rate=max(np.random.normal(self.response_rate, std_dev*self.response_rate), 0.005),\n",
    "                    response_start=max(np.random.normal(self.response_start, math.floor(std_dev*self.response_start)), 0), \n",
    "                    base_spread_rate=max(np.random.normal(self.base_spread_rate, std_dev*self.base_spread_rate), 0),\n",
    "                )\n",
    "            else:\n",
    "                simulator = Simulator(size=self.size, wind_speed=self.wind_speed, wind_direction=self.wind_direction, response_rate=self.response_rate, \n",
    "                    response_start=self.response_start, base_spread_rate=self.base_spread_rate)\n",
    "                \n",
    "            simulator.simulate()\n",
    "\n",
    "            simulation_data = []\n",
    "            past_info = np.zeros((self.size, self.size, 2))\n",
    "            for t in simulator.maps:\n",
    "                if 0.5 > np.random.rand():\n",
    "                    # sample a local patch where an agent observed the previous map\n",
    "                    coords = np.random.randint(0, self.size, 2)   # FIX: use grid size, not hard-coded 256\n",
    "                    if t>0:\n",
    "                        prev_map = simulator.maps[t-1]\n",
    "                        y_min, y_max = max(0, coords[0]-5), min(self.size, coords[0]+6)\n",
    "                        x_min, x_max = max(0, coords[1]-5), min(self.size, coords[1]+6)\n",
    "\n",
    "                        past_info[y_min:y_max, x_min:x_max, 0] = prev_map[y_min:y_max, x_min:x_max]\n",
    "                        past_info[y_min:y_max, x_min:x_max, 1] = 0     \n",
    "\n",
    "                    past_info[:, :, 1] += 1\n",
    "\n",
    "                \n",
    "                data_point = self.dataset.generate_data(topology=simulator.terrain, past_info=past_info, wind_direction=np.array(simulator.wind_direction), \n",
    "                                           wind_speed=simulator.wind_speed, time=t, label=simulator.maps[t])\n",
    "                simulation_data.append(data_point)\n",
    "                \n",
    "            self.dataset.save_data(np.array(simulation_data))\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, size=128, model_layers=10, lr=1e-4, num_output_classes=6, model_dir=None):\n",
    "        self.generator = DatasetGenerator(size=size)\n",
    "\n",
    "        self.model = BelieverModel(num_layers=model_layers, num_output_classes=num_output_classes, nodes=size**2)\n",
    "        if model_dir:\n",
    "            self.model.load_state_dict(torch.load(model_dir))\n",
    "            self.model_dir = model_dir\n",
    "        else:\n",
    "            self.model_dir = f\"model/model{size}.pth\"\n",
    "\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        \n",
    "\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss() # ordinal labels\n",
    "\n",
    "    def generate(self, num_sims=100):\n",
    "        '''\n",
    "        Generates new simulations to be used to train.\n",
    "        '''\n",
    "\n",
    "        self.generator.generate(num_sims=num_sims)\n",
    "        print(f'Generated and saved {num_sims} simulations.')\n",
    "\n",
    "    def clear_data(self):\n",
    "        dir = self.generator.dataset.save_dir\n",
    "\n",
    "        for f in os.listdir(dir):\n",
    "            if f.endswith(\".npy\"):\n",
    "                os.remove(os.path.join(dir, f))\n",
    "\n",
    "    def train_without_replacement(self, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "              num_sims=100, num_epochs=100):\n",
    "        '''\n",
    "        Generates new simulations for each epoch and clears old simulations to reduce overfitting risk.\n",
    "        '''\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.generate(num_sims=num_sims)\n",
    "\n",
    "            self.generator.dataset.generate_dataset()\n",
    "            print(f\"Generated the simulations for {epoch+1}/{num_epochs}.\")\n",
    "            train_loader = DataLoader(self.generator.dataset, batch_size=32, shuffle=True,\n",
    "                                      num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_nodes = 0\n",
    "\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                B, N, F = batch_x.shape\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                flat_x = batch_x.reshape(B * N, F)\n",
    "                adj_batch = block_diag_batch(self.generator.dataset.adjacency_matrix.to(device), B)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model(flat_x, adj_batch)\n",
    "\n",
    "                loss = self.loss_fn(\n",
    "                    logits, \n",
    "                    batch_y.reshape(-1)\n",
    "                ) # averaged over all nodes in all maps\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                total_correct += (preds == batch_y.reshape(-1)).sum().item()\n",
    "                total_nodes += batch_y.numel()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            acc = total_correct / total_nodes\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "            self.clear_data()\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "        torch.save(self.model.state_dict(), self.model_dir)\n",
    "    \n",
    "    def train_with_replacement(self, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), num_epochs=10):\n",
    "        '''\n",
    "        Retrains on past completed simulations, without generating new ones.\n",
    "        '''\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            self.generator.dataset.generate_dataset()\n",
    "            train_loader = DataLoader(self.generator.dataset, batch_size=32, shuffle=True,\n",
    "                  num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "            \n",
    "            print(f\"Generated the dataset for epoch {epoch+1}/{num_epochs}.\")\n",
    "\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_nodes = 0\n",
    "\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                B, N, F = batch_x.shape\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                flat_x = batch_x.reshape(B * N, F)\n",
    "                adj_batch = block_diag_batch(self.generator.dataset.adjacency_matrix.to(device), B)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model(flat_x, adj_batch)\n",
    "\n",
    "                loss = self.loss_fn(\n",
    "                    logits, \n",
    "                    batch_y.reshape(-1)\n",
    "                ) # averaged over all nodes in all maps\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                total_correct += (preds == batch_y.reshape(-1)).sum().item()\n",
    "                total_nodes += batch_y.numel()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            acc = total_correct / total_nodes   \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "        torch.save(self.model.state_dict(), self.model_dir)\n",
    "\n",
    "    def generate_comparison_gif(self, filename=\"comparison.gif\", cmap=\"plasma\", \n",
    "                                device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                                new_sim = False):\n",
    "        '''\n",
    "        Generates a gif to compare truth, model knowledge, and model prediction as the situation evolves.\n",
    "        '''\n",
    "\n",
    "        if new_sim:\n",
    "            self.generate(num_sims=1)\n",
    "\n",
    "        dataset = self.generator.dataset\n",
    "        files = [f for f in os.listdir(dataset.save_dir) if f.endswith('.npy')]\n",
    "        if not files:\n",
    "            raise RuntimeError(f\"No simulation files found in {dataset.save_dir}\")\n",
    "\n",
    "        latest = max(files, key=lambda f: os.path.getmtime(os.path.join(dataset.save_dir, f)))\n",
    "        sim_path = os.path.join(dataset.save_dir, latest)\n",
    "\n",
    "        sim = np.load(sim_path)\n",
    "        T = sim.shape[0]\n",
    "        N = sim.shape[1]\n",
    "        size = int(np.sqrt(N))\n",
    "\n",
    "        terrain = sim[0, :, 0].reshape(size, size)\n",
    "\n",
    "        past_info_seq = sim[:, :, 1].reshape(T, size, size)\n",
    "        past_info_date_seq = sim[:, :, 2].reshape(T, size, size)\n",
    "\n",
    "        truth_seq = sim[:, :, -1].reshape(T, size, size)\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Compute model predictions for each timestep\n",
    "        pred_seq = []\n",
    "        with torch.no_grad():\n",
    "            for t in range(T):\n",
    "                x_t = torch.from_numpy(sim[t, :, :-1]).float().to(device)  # (N, F)\n",
    "                logits = self.model(x_t, dataset.adjacency_matrix.to(device))\n",
    "                pred_map = logits.argmax(dim=1).cpu().numpy().reshape(size, size)\n",
    "                pred_seq.append(pred_map)\n",
    "        pred_seq = np.array(pred_seq)\n",
    "\n",
    "        # compute vmin/vmax for fire panels (truth + model)\n",
    "        vmin_fire = float(min(truth_seq.min(), pred_seq.min()))\n",
    "        vmax_fire = float(max(truth_seq.max(), pred_seq.max()))\n",
    "\n",
    "        # compute vmin/vmax for knowledge panel\n",
    "        vmin_k = float(past_info_seq.min())\n",
    "        vmax_k = float(past_info_seq.max())\n",
    "\n",
    "        if vmax_k <= 0:\n",
    "            knowledge_vis_scale = 1.0\n",
    "        else:\n",
    "            knowledge_vis_scale = max(1.0, (0.8 * (vmax_fire + 1e-9)) / (vmax_k))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        panels = [\"Truth\", \"Knowledge\", \"Model Prediction\"]\n",
    "        fire_imgs = []\n",
    "        terrain_imgs = []\n",
    "\n",
    "        for ax, title in zip(axes, panels):\n",
    "            ax.set_title(title)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            if title == \"Knowledge\":\n",
    "                fire_img = ax.imshow(np.zeros((size, size)), cmap=cmap, vmin=vmin_k * knowledge_vis_scale, vmax=vmax_k * knowledge_vis_scale)\n",
    "            else:\n",
    "                fire_img = ax.imshow(np.zeros((size, size)), cmap=cmap, vmin=vmin_fire, vmax=vmax_fire)\n",
    "            fire_imgs.append(fire_img)\n",
    "\n",
    "            terrain_img = ax.imshow(terrain, cmap=\"Greens\", alpha=0.3)\n",
    "            terrain_imgs.append(terrain_img)\n",
    "\n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "        from matplotlib import colors\n",
    "        fig.colorbar(fire_imgs[0], cax=cbar_ax)\n",
    "\n",
    "        max_frame = truth_seq.shape[0]\n",
    "\n",
    "        def update(frame_idx):\n",
    "            fire_imgs[0].set_data(truth_seq[frame_idx])\n",
    "\n",
    "            # Knowledge\n",
    "            pinfo = past_info_seq[frame_idx]\n",
    "            pdate = past_info_date_seq[frame_idx]\n",
    "            tau = 10.0\n",
    "            fading_map = pinfo * np.exp(-pdate / tau)\n",
    "            fading_vis = fading_map * knowledge_vis_scale\n",
    "            fire_imgs[1].set_data(fading_vis)\n",
    "\n",
    "            # Model predictions for this timestep\n",
    "            fire_imgs[2].set_data(pred_seq[frame_idx])\n",
    "\n",
    "            return fire_imgs + terrain_imgs\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, update, frames=max_frame, interval=200, blit=False)\n",
    "        ani.save(filename, writer=\"pillow\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), self.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67e960f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _worker_simulate_and_save(worker_idx, size, base_kwargs, save_dir):\n",
    "    np.random.seed(int(time.time()) ^ (os.getpid() << 16) ^ worker_idx)  # reseed\n",
    "    sim = Simulator(size=size,\n",
    "                    wind_speed=base_kwargs.get('wind_speed', 1.5),\n",
    "                    wind_direction=base_kwargs.get('wind_direction', [1,2]),\n",
    "                    response_rate=base_kwargs.get('response_rate', 0.03),\n",
    "                    response_start=base_kwargs.get('response_start', 100),\n",
    "                    base_spread_rate=base_kwargs.get('base_spread_rate', 0.03),\n",
    "                    n_components=base_kwargs.get('n_components', None),\n",
    "                    decay_rate=base_kwargs.get('decay_rate', 1e-3))\n",
    "    sim.simulate()\n",
    "\n",
    "    dataset = FireGraph(length=size, width=size)\n",
    "    simulation_data = []\n",
    "    past_info = np.zeros((size, size, 2), dtype=np.float32)\n",
    "    for t in sorted(sim.maps.keys()):\n",
    "        if 0.5 > np.random.rand():\n",
    "            coords = np.random.randint(0, size, 2)\n",
    "            if t > 0:\n",
    "                prev_map = sim.maps[t-1]\n",
    "                y_min, y_max = max(0, coords[0]-5), min(size, coords[0]+6)\n",
    "                x_min, x_max = max(0, coords[1]-5), min(size, coords[1]+6)\n",
    "                past_info[y_min:y_max, x_min:x_max, 0] = prev_map[y_min:y_max, x_min:x_max]\n",
    "                past_info[y_min:y_max, x_min:x_max, 1] = 0\n",
    "            past_info[:, :, 1] += 1\n",
    "\n",
    "        dp = dataset.generate_data(topology=sim.terrain,\n",
    "                                   past_info=past_info,\n",
    "                                   wind_direction=np.array(sim.wind_direction),\n",
    "                                   wind_speed=sim.wind_speed,\n",
    "                                   time=t,\n",
    "                                   label=sim.maps[t])\n",
    "        simulation_data.append(dp)\n",
    "\n",
    "    out = np.array(simulation_data)\n",
    "\n",
    "    # unique filename: timestamp + pid + worker idx\n",
    "    fname_base = f\"{int(time.time()):d}_{os.getpid()}_{worker_idx}\"\n",
    "    p = os.path.join(save_dir, fname_base + '.npy')\n",
    "    np.save(p, out)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel simulation generator using ProcessPoolExecutor\n",
    "\n",
    "def generate_parallel(num_sims=100, size=128, base_kwargs=None):\n",
    "    if base_kwargs is None:\n",
    "        base_kwargs = {}\n",
    "    save_dir = os.path.join('simulation_data', f\"{size}x{size}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "  \n",
    "    available_cpus = multiprocessing.cpu_count()\n",
    "    max_workers = max(1, available_cpus - 1)\n",
    "\n",
    "    futures = []\n",
    "    created = []\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as exe:\n",
    "        for i in range(num_sims):\n",
    "            futures.append(exe.submit(_worker_simulate_and_save, i, size, base_kwargs, save_dir))\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                path = fut.result()\n",
    "                created.append(path)\n",
    "            except Exception as e:\n",
    "                print('Worker failed:', e)\n",
    "\n",
    "    print(f'Finished. Created {len(created)} files in {save_dir}')\n",
    "    return created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b772d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/mreisigahrooei/mtaskin/serhans_venv/lib/python3.10/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"model/model.pth\"\n",
    "trainer = Trainer(model_dir = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc77f897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/mreisigahrooei/mtaskin/serhans_venv/lib/python3.10/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing save_dir to ensure clean dataset: simulation_data/16x16\n",
      "Starting Trainer.train_with_replacement for 10 epoch(s) on device=cpu\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def run_overfit_with_replacement(size=64, num_sims=3, num_epochs=10, lr=1e-3, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Use Trainer.generate(...) then Trainer.train_with_replacement(...) to test overfitting.\n",
    "\n",
    "    - size: grid dimension (size x size)\n",
    "    - num_sims: how many simulation files to create (keep small to make memorization easier)\n",
    "    - num_epochs: number of epochs passed to Trainer.train_with_replacement\n",
    "    - lr: learning rate (passed to Trainer constructor)\n",
    "    - device: torch device to run on\n",
    "\n",
    "    Returns the trainer instance after training for inspection.\n",
    "    \"\"\"\n",
    "    # deterministic seeds for reproducibility\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    trainer = Trainer(size=size, model_layers=3, lr=lr, num_output_classes=6, model_dir=None)\n",
    "    save_dir = trainer.generator.dataset.save_dir\n",
    "\n",
    "    # Clear any old sims so we start clean\n",
    "    if os.path.isdir(save_dir):\n",
    "        print('Removing existing save_dir to ensure clean dataset:', save_dir)\n",
    "        shutil.rmtree(save_dir)\n",
    "\n",
    "    # Ensure save_dir exists again so Trainer.generate can write files\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Call the class method you wrote to train on the generated sims (with replacement)\n",
    "    print(f'Starting Trainer.train_with_replacement for {num_epochs} epoch(s) on device={device}')\n",
    "    trainer.train_without_replacement(device=device, num_epochs=num_epochs, num_sims=num_sims)\n",
    "\n",
    "    print('Trainer.train_with_replacement completed. Inspect trainer or saved model as needed.')\n",
    "    return trainer\n",
    "\n",
    "# Usage example (uncomment to run):\n",
    "trainer = run_overfit_with_replacement(size=16, num_sims=10, num_epochs=10, lr=1e-3, device=torch.device('cpu'))\n",
    "\n",
    "trainer.generate_comparison_gif(new_sim=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "serhans_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
