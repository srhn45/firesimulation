{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4252f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from math import exp, sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9545a3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from scipy.sparse import lil_matrix\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a622a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture_field(size, n_components=None):\n",
    "    x, y = np.meshgrid(np.linspace(0, 1, size), np.linspace(0, 1, size))\n",
    "    field = np.zeros((size, size))\n",
    "\n",
    "    if not n_components:\n",
    "        n_components = np.random.poisson(20)\n",
    "\n",
    "    for _ in range(n_components):\n",
    "        cx, cy = np.random.uniform(0, 1, 2) # random center\n",
    "        sx, sy = np.random.uniform(0.01, 0.2, 2) # random covariance scale\n",
    "        w = np.random.exponential(10) # weight scale\n",
    "\n",
    "        gaussian = w * np.exp(-(((x - cx) ** 2) / (2 * sx**2) +\n",
    "                                ((y - cy) ** 2) / (2 * sy**2)))\n",
    "        field += gaussian\n",
    "\n",
    "    field = (1 - (field - field.min()) / (field.max() - field.min()))*2\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a0e3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator():\n",
    "    def __init__(self, size=256, wind_speed=0, wind_direction=[0,0], response_rate=0.1, response_start=20, base_spread_rate=0.3, n_components=None, decay_rate=1e-3):\n",
    "        self.size = size\n",
    "        self.map = np.zeros((size, size))\n",
    "        self.wind_speed = wind_speed\n",
    "        self.wind_direction = wind_direction\n",
    "        self.response_rate = response_rate\n",
    "        self.response_start = response_start\n",
    "        self.spread_rate = base_spread_rate\n",
    "        self.time = 0\n",
    "        self.decay_rate = decay_rate\n",
    "        self.maps = {}\n",
    "\n",
    "        self.terrain = gaussian_mixture_field(size, n_components=n_components)\n",
    "\n",
    "    def step(self):\n",
    "        new_map = deepcopy(self.map)\n",
    "\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if self.map[i, j] >= 1:\n",
    "                    if np.random.rand() < self.spread_rate*self.terrain[i, j]*np.exp(-self.decay_rate * self.time) and new_map[i, j] < 5:\n",
    "                        new_map[i, j] += 1\n",
    "                    \n",
    "                    for di in [-1, 0, 1]:\n",
    "                        for dj in [-1, 0, 1]:\n",
    "                            if di == 0 and dj == 0:\n",
    "                                continue\n",
    "\n",
    "                            ni, nj = i + di, j + dj\n",
    "                            spread_chance = self.spread_rate*self.map[i,j]\n",
    "\n",
    "                            if 0 <= ni < self.size and 0 <= nj < self.size:\n",
    "                                if self.wind_speed > 0:\n",
    "                                    wind_influence = (di * self.wind_direction[0] + dj * self.wind_direction[1]) / (np.linalg.norm(self.wind_direction) + 1e-6)\n",
    "                                    wind_influence *= np.random.normal(1, 0.5)\n",
    "\n",
    "\n",
    "                                    if wind_influence > 0:\n",
    "                                        spread_chance *= (1 + self.wind_speed * wind_influence)\n",
    "                                    spread_chance *= self.terrain[ni, nj]\n",
    "                                    spread_chance *= np.exp(-self.decay_rate * self.time)\n",
    "                                    spread_chance = np.clip(spread_chance, 0, 1)\n",
    "\n",
    "                                if np.random.rand() < spread_chance and new_map[ni, nj] <= new_map[i, j]:\n",
    "                                    if new_map[ni, nj] < 5:\n",
    "                                        if np.random.rand() <= exp(-self.time/1000):\n",
    "                                            new_map[ni, nj] += 1\n",
    "\n",
    "                                if self.time >= self.response_start and new_map[ni, nj] == 0:\n",
    "                                    if np.random.rand() < 1 - exp(-(self.response_rate*(0.5+self.terrain[i,j]) * (self.time - self.response_start))):\n",
    "                                        if new_map[i, j] > 0:\n",
    "                                            new_map[i, j] -= 1 # Firefighting effort\n",
    "                            if np.exp(-self.decay_rate * self.time) < 0.5:\n",
    "                                if ni < 0 or ni >= self.size or nj < 0 or nj >= self.size:\n",
    "                                    if np.random.rand() < 1 - exp(-(self.response_rate) * (self.time - self.response_start)):\n",
    "                                        if new_map[i, j] > 0:\n",
    "                                            new_map[i, j] -= 1 # Edge effect\n",
    "                    \n",
    "                else:\n",
    "                    if 1 < i < self.size - 1 and 1 < j < self.size - 1:\n",
    "                        neighbors_on_fire = np.sum(self.map[i-1:i+2, j-1:j+2] >= 1) - (1 if self.map[i, j] >= 1 else 0)\n",
    "                        if neighbors_on_fire >= 6 and new_map[i, j] == 0:\n",
    "                            new_map[i, j] += 1\n",
    "        \n",
    "        self.maps[self.time] = deepcopy(self.map)\n",
    "        self.map = new_map\n",
    "        self.time += 1\n",
    "\n",
    "    def simulate(self):\n",
    "        nodes = np.random.poisson(3)\n",
    "\n",
    "        x_init, y_init = np.random.randint(0, self.size, size=2)\n",
    "        self.map[x_init, y_init] = np.random.poisson(3)\n",
    "\n",
    "        for _ in range(nodes - 1):\n",
    "            while True:\n",
    "                x, y = np.random.randint(-20, 21, size=2)\n",
    "                if x_init+x < self.size and y_init+y < self.size:\n",
    "                    break\n",
    "                \n",
    "            if self.map[x_init+x, y_init+y] == 0:\n",
    "                self.map[x_init+x, y_init+y] = np.random.poisson(3)\n",
    "                break\n",
    "    \n",
    "        while np.any(self.map > 0):\n",
    "            self.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1fba65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gaussian_mixture_field' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m simulator \u001b[38;5;241m=\u001b[39m \u001b[43mSimulator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwind_speed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwind_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_spread_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m simulator\u001b[38;5;241m.\u001b[39msimulate()\n",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36mSimulator.__init__\u001b[0;34m(self, size, wind_speed, wind_direction, response_rate, response_start, base_spread_rate, n_components, decay_rate)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecay_rate \u001b[38;5;241m=\u001b[39m decay_rate\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaps \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterrain \u001b[38;5;241m=\u001b[39m \u001b[43mgaussian_mixture_field\u001b[49m(size, n_components\u001b[38;5;241m=\u001b[39mn_components)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gaussian_mixture_field' is not defined"
     ]
    }
   ],
   "source": [
    "#simulator = Simulator(size=256, wind_speed=1.5, wind_direction=[1,2], response_rate=0.05, response_start=100, base_spread_rate=0.05)\n",
    "#simulator.simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d70e6035",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simulator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     ani\u001b[38;5;241m.\u001b[39msave(filename, writer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpillow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose(fig)\n\u001b[0;32m---> 28\u001b[0m make_heatmap_gif(\u001b[43msimulator\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfire.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'simulator' is not defined"
     ]
    }
   ],
   "source": [
    "def make_heatmap_gif(simulator, filename=\"simulation.gif\", cmap=\"plasma\"):\n",
    "    times = sorted(simulator.maps.keys())\n",
    "    frames = [simulator.maps[t] for t in times]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # --- fire as background ---\n",
    "    vmin, vmax = np.min(frames), np.max(frames)\n",
    "    fire_img = ax.imshow(frames[0], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # --- terrain overlay ---\n",
    "    terrain_img = ax.imshow(simulator.terrain, cmap=\"Greens\", alpha=0.2)  # low alpha on top\n",
    "\n",
    "    cbar = fig.colorbar(fire_img, ax=ax)\n",
    "    cbar.set_label(\"Fire Intensity\", rotation=270, labelpad=15)\n",
    "\n",
    "    def update(frame):\n",
    "        fire_img.set_data(frame)    \n",
    "        return [fire_img, terrain_img]\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, update, frames=frames, interval=60, blit=True\n",
    "    )\n",
    "\n",
    "    ani.save(filename, writer=\"pillow\")\n",
    "    plt.close(fig)\n",
    "\n",
    "make_heatmap_gif(simulator, \"fire.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "926e290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One approach could be using GNN's, which would result in a scalable network to different grid sizes\n",
    "# Another is to use a transformer, easier to train\n",
    "# I'm going to try a Graph Attention Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa9430ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_matrix(length, width):\n",
    "    N = length * width\n",
    "    adj = lil_matrix((N, N), dtype=np.float32)\n",
    "    directions = [(-1,0), (-1,1), (0,1), (1,1), (1,0), (1,-1), (0,-1), (-1,-1), (0,0)] # 8 sided\n",
    "    for i in range(N):\n",
    "        x, y = divmod(i, width)\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x+dx, y+dy\n",
    "            if 0 <= nx < length and 0 <= ny < width:\n",
    "                j = nx*width + ny\n",
    "                adj[i,j] = 1\n",
    "    return adj.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9f7d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireGraph(Dataset):\n",
    "    def __init__(self, length=256, width=256, path=\"simulation_data\"):\n",
    "        self.path = path\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "\n",
    "\n",
    "        adj = adjacency_matrix(self.length, self.width)\n",
    "        self.adjacency_matrix = torch.sparse_coo_tensor(\n",
    "            indices=torch.tensor(np.vstack((adj.row, adj.col)), dtype=torch.long),\n",
    "            values=torch.tensor(adj.data, dtype=torch.float32),\n",
    "            size=adj.shape\n",
    "        )\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "        self.save_dir = os.path.join(self.path, f\"{self.length}x{self.width}\")\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def generate_data(self, topology:np.array=None, past_info:np.array=None, wind_direction:np.array=np.array([0,0]),\n",
    "                         wind_speed:int=0, time:int=0, label:np.array=None):\n",
    "        \n",
    "        flat_topo = topology.ravel()\n",
    "        flat_info = past_info[:, :, 0].ravel()\n",
    "        flat_info_date = past_info[:, :, 1].ravel()\n",
    "        flat_label = label.ravel()\n",
    "        \n",
    "        data = np.stack([\n",
    "            flat_topo,\n",
    "            flat_info,\n",
    "            flat_info_date,\n",
    "            np.full(flat_topo.shape, wind_direction[0], dtype=np.float32),\n",
    "            np.full(flat_topo.shape, wind_direction[1], dtype=np.float32),\n",
    "            np.full(flat_topo.shape, wind_speed, dtype=np.float32),\n",
    "            np.full(flat_topo.shape, time, dtype=np.float32),\n",
    "            flat_label\n",
    "        ], axis=1)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def save_data(self, data:np.array):\n",
    "        np.save(os.path.join(self.save_dir, f\"{time.time():.0f}.npy\"), data)\n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        arrays = []\n",
    "        for file in os.listdir(self.save_dir):\n",
    "            if file.endswith(\".npy\"):\n",
    "                arr = np.load(os.path.join(self.save_dir, file))\n",
    "                arrays.append(arr)\n",
    "\n",
    "        if arrays:\n",
    "            self.data = np.concatenate(arrays, axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        arr = self.data[idx]  # arr shape = (N, F+1)\n",
    "        x = arr[:, :-1]       # (N, F)\n",
    "        y = arr[:, -1]        # (N,)\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(y).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "401dcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced https://github.com/gordicaleksa/pytorch-GAT/blob/main/The%20Annotated%20GAT%20(Cora).ipynb for some of the code.\n",
    "\n",
    "class BelieverModel(nn.Module):\n",
    "    def __init__(self, nodes=256*256, input_features=7, num_layers=3, num_heads=3, num_features_per_head=4, num_output_classes=5, dropout=False):\n",
    "        super().__init__()\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = False\n",
    "        self.N = nodes\n",
    "        self.num_heads = num_heads\n",
    "        self.num_features_per_head = num_features_per_head\n",
    "\n",
    "        self.initial_transformation = nn.Linear(input_features, num_heads * num_features_per_head, bias=False)\n",
    "        nn.init.xavier_normal_(self.initial_transformation.weight)\n",
    "\n",
    "        self.a_lefts = nn.ParameterList()\n",
    "        self.a_rights = nn.ParameterList()\n",
    "        self.Ws = nn.ParameterList()\n",
    "        for i in range(num_layers):\n",
    "            a_left = nn.Parameter(torch.zeros(size=(num_heads, num_features_per_head)))\n",
    "            nn.init.xavier_uniform_(a_left)\n",
    "            a_right = nn.Parameter(torch.zeros(size=(num_heads, num_features_per_head)))\n",
    "            nn.init.xavier_uniform_(a_right)\n",
    "            W = nn.Parameter(torch.zeros(size=(num_heads, num_features_per_head, num_features_per_head)))\n",
    "            nn.init.xavier_normal_(W)\n",
    "\n",
    "            self.a_lefts.append(a_left)\n",
    "            self.a_rights.append(a_right)\n",
    "            self.Ws.append(W)\n",
    "        \n",
    "        self.final_transformation = nn.Linear(num_heads*num_features_per_head, num_output_classes)\n",
    "        nn.init.xavier_normal_(self.final_transformation.weight)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # adj = (N, N) adjacency matrix, in coo matrix format\n",
    "        # x = inputs (N, F_in)\n",
    "\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        N = x.size(0)\n",
    "        x = self.initial_transformation(x) # (N, F_out*H)\n",
    "        x = x.view(N, self.num_heads, self.num_features_per_head) # (N, H, F_out)\n",
    "\n",
    "        for W, a_left, a_right in zip(self.Ws, self.a_lefts, self.a_rights):\n",
    "            # W = (H, F_out, F_out)\n",
    "            # a_left = (H, F_out)\n",
    "            # a_right = (H, F_out)\n",
    "\n",
    "            # alpha_i,j = exp(a * [h_i||h_j] * adj[i,j]) / sum_j(exp(a * [h_i||h_j] * adj[i,j])), softmax\n",
    "            # h(i') = adj * sum_j (alpha_i,j * W * h_j)\n",
    "            # to simplify, we split a into 2 parts a_left and a_right, and calculate the attention scores for each of those parts, then sum up the scores only for viable pairs for computational efficiency\n",
    "\n",
    "            h_prime = torch.einsum(\"nhf,hfo->nho\", x, W) # (N, H, F_out) x (H, F_out, F_out) -> (N, H, F_out)\n",
    "\n",
    "            source_scores = (h_prime * a_left).sum(-1) # elementwise product, (N, H)\n",
    "            neighbor_scores = (h_prime * a_right).sum(-1) # (N, H)\n",
    "\n",
    "            adj = adj.coalesce()\n",
    "            row, col = adj.indices()\n",
    "            row = row.long(); col = col.long()\n",
    "            e = self.leakyrelu(source_scores[row] + neighbor_scores[col]) # (E, H) where E is the number of edges\n",
    "\n",
    "            H =e.size(1)\n",
    "            if hasattr(torch.Tensor, \"scatter_reduce\"):\n",
    "                max_per_node = torch.zeros((N, H), device=e.device, dtype=e.dtype).scatter_reduce(\n",
    "                    0, row.unsqueeze(-1).expand(-1,H), e, reduce=\"amax\", include_self=False\n",
    "                ) # maximum score among all neighbors of node i, (N, H)\n",
    "            else:\n",
    "                # fallback: compute max per node manually (safe but slower)\n",
    "                max_per_node = torch.full((N,H), -1e9, device=e.device, dtype=e.dtype)\n",
    "                for i_edge, i_node in enumerate(row):\n",
    "                    max_per_node[i_node] = torch.maximum(max_per_node[i_node], e[i_edge])\n",
    "            exp_e = torch.exp(e - max_per_node[row]) # for numerical stability, (E, H)\n",
    "\n",
    "            denom = torch.zeros((N, H), device=e.device, dtype=e.dtype)\n",
    "            denom.index_add_(0, row, exp_e) # summation over neighbors, (N, H)\n",
    "\n",
    "            alpha = exp_e / (denom[row] + 1e-9) # elementwise division, (E, H)\n",
    "\n",
    "            messages = h_prime[col] * alpha.unsqueeze(-1) # (E, H, F)\n",
    "\n",
    "            out = torch.zeros_like(h_prime, device=h_prime.device, dtype=h_prime.dtype)\n",
    "            out.index_add_(0, row, messages) # summation over neighbors again, (N, H, F)\n",
    "            x = self.relu(out)\n",
    "        \n",
    "        x = x.reshape(N, self.num_heads * self.num_features_per_head)\n",
    "        logits = self.final_transformation(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47fd6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator():\n",
    "    def __init__(self, size=256, wind_speed=1.5, wind_direction=[1,2], response_rate=0.05, response_start=100, base_spread_rate=0.05, perturb=True):\n",
    "        self.size = size\n",
    "        self.wind_speed = wind_speed\n",
    "        self.wind_direction = wind_direction\n",
    "        self.response_rate = response_rate\n",
    "        self.response_start = response_start\n",
    "        self.base_spread_rate = base_spread_rate\n",
    "        self.perturb = perturb\n",
    "        self.dataset = FireGraph(length=self.size, width=self.size)\n",
    "\n",
    "    def generate(self, num_sims=100, std_dev=0.2):\n",
    "        for i in range(num_sims):\n",
    "            if self.perturb:\n",
    "                \n",
    "                simulator = Simulator(size=self.size, wind_speed=np.random.normal(self.wind_speed, std_dev*self.wind_speed), wind_direction=[np.random.uniform(-1, 1), np.random.uniform(-1,1)], \n",
    "                    response_rate=max(np.random.normal(self.response_rate, std_dev*self.response_rate), 0.005),\n",
    "                    response_start=max(np.random.normal(self.response_start, math.floor(std_dev*self.response_start)), 0), \n",
    "                    base_spread_rate=max(np.random.normal(self.base_spread_rate, std_dev*self.base_spread_rate), 0),\n",
    "                )\n",
    "            else:\n",
    "                simulator = Simulator(size=self.size, wind_speed=self.wind_speed, wind_direction=self.wind_direction, response_rate=self.response_rate, \n",
    "                    response_start=self.response_start, base_spread_rate=self.base_spread_rate)\n",
    "                \n",
    "            simulator.simulate()\n",
    "\n",
    "            simulation_data = []\n",
    "            past_info = np.zeros((self.size, self.size, 2))\n",
    "            for t in simulator.maps:\n",
    "                if 0.5 > np.random.rand():\n",
    "                    coords = np.random.randint(0, 256, 2)\n",
    "                    if t>0:\n",
    "                        prev_map = simulator.maps[t-1]\n",
    "                        y_min, y_max = max(0, coords[0]-3), min(self.size, coords[0]+4)\n",
    "                        x_min, x_max = max(0, coords[1]-3), min(self.size, coords[1]+4)\n",
    "\n",
    "                        past_info[y_min:y_max, x_min:x_max, 0] = prev_map[y_min:y_max, x_min:x_max]\n",
    "                        past_info[y_min:y_max, x_min:x_max, 1] = 0     \n",
    "\n",
    "                    past_info[:, :, 1] += 1\n",
    "\n",
    "                \n",
    "                data_point = self.dataset.generate_data(topology=simulator.terrain, past_info=past_info, wind_direction=np.array(simulator.wind_direction), \n",
    "                                           wind_speed=simulator.wind_speed, time=t, label=simulator.maps[t])\n",
    "                simulation_data.append(data_point)\n",
    "                \n",
    "            self.dataset.save_data(np.array(simulation_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97c7f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, size=256, model_layers=10, lr=1e-4, num_output_classes=5, model_dir=None):\n",
    "        self.generator = DatasetGenerator(size=size)\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        self.model = BelieverModel(num_layers=model_layers, num_output_classes=num_output_classes, nodes=size**2)\n",
    "        if model_dir:\n",
    "            self.model.load_state_dict(torch.load(model_dir))\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss() # ordinal labels\n",
    "\n",
    "    def generate(self, num_sims=100):\n",
    "        self.generator.generate(num_sims=num_sims)\n",
    "\n",
    "    def clear_data(self):\n",
    "        dir = self.generator.dataset.save_dir\n",
    "\n",
    "        for f in os.listdir(dir):\n",
    "            if f.endswith(\".npy\"):\n",
    "                os.remove(os.path.join(self.save_dir, f))\n",
    "\n",
    "    def train(self, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "              num_sims=100, num_epochs=100):\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.generate(num_sims=num_sims)\n",
    "\n",
    "            self.generator.dataset.generate_dataset()\n",
    "            print(f\"Generated the simulations for {epoch+1}/{num_epochs}.\")\n",
    "            train_loader = DataLoader(self.generator.dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_nodes = 0\n",
    "\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                B, N, F = batch_x.shape\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                flat_x = batch_x.reshape(B * N, F)\n",
    "                adj_batch = block_diag_batch(self.generator.dataset.adjacency_matrix.to(device), B)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model(flat_x, adj_batch)\n",
    "\n",
    "                loss = self.loss_fn(\n",
    "                    logits, \n",
    "                    batch_y.reshape(-1)\n",
    "                ) # averaged over all nodes in all maps\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                total_correct += (preds == batch_y).sum().item()\n",
    "                total_nodes += batch_y.numel()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            acc = total_correct / total_nodes\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "            self.clear_data()\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "        torch.save(self.model.state_dict(), self.model_dir)\n",
    "\n",
    "    def generate_comparison_gif(self, filename=\"comparison.gif\", cmap=\"plasma\", \n",
    "                                device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "):\n",
    "        self.generate(num_sims=1)\n",
    "        dataset = self.generator.dataset\n",
    "\n",
    "        dataset.generate_dataset()\n",
    "        x_full, y_full = dataset[0]\n",
    "\n",
    "        size = int(np.sqrt(x_full.shape[0]))\n",
    "        terrain = x_full[:, 0].reshape(size, size)\n",
    "        past_info = x_full[:, 1].reshape(size, size)\n",
    "        past_info_date = x_full[:, 2].reshape(size, size)\n",
    "        truth = y_full.reshape(size, size)\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(x_full.to(device), dataset.adjacency_matrix.to(device))\n",
    "            pred_map = logits.argmax(dim=1).cpu().numpy().reshape(size, size)\n",
    "\n",
    "        vmin, vmax = min(truth.min(), pred_map.min(), past_info.min()), max(truth.max(), pred_map.max(), past_info.max())\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        panels = [\"Truth\", \"Knowledge (fading)\", \"Model Prediction\"]\n",
    "        fire_imgs = []\n",
    "        terrain_imgs = []\n",
    "\n",
    "        for ax, title in zip(axes, panels):\n",
    "            ax.set_title(title)\n",
    "            ax.axis(\"off\")\n",
    "            fire_img = ax.imshow(np.zeros((size, size)), cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "            fire_imgs.append(fire_img)\n",
    "            terrain_img = ax.imshow(terrain, cmap=\"Greens\", alpha=0.2)\n",
    "            terrain_imgs.append(terrain_img)\n",
    "\n",
    "        def update(frame_idx):\n",
    "            # Truth\n",
    "            fire_imgs[0].set_data(truth)\n",
    "\n",
    "            # Knowledge, fading over time\n",
    "            fading_map = np.where(past_info_date <= frame_idx, past_info,\n",
    "                                past_info * np.exp(-(past_info_date - frame_idx)/10))\n",
    "            fire_imgs[1].set_data(fading_map)\n",
    "\n",
    "            # Model predictions\n",
    "            fire_imgs[2].set_data(pred_map)\n",
    "\n",
    "            return fire_imgs + terrain_imgs\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, update, frames=int(past_info_date.max())+1, interval=200, blit=True)\n",
    "        ani.save(filename, writer=\"pillow\")\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42c8f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_diag_batch(adj, batch_size):\n",
    "    \"\"\"\n",
    "    Build block-diagonal adjacency matrix for efficient processing of batches.\n",
    "    \"\"\"\n",
    "    N = adj.size(0)\n",
    "    indices = adj.indices()\n",
    "    values = adj.values()\n",
    "\n",
    "    offsets = torch.arange(batch_size, device=indices.device) * N\n",
    "    offsets = offsets.view(1, -1, 1)\n",
    "\n",
    "    expanded = indices.unsqueeze(1).expand(-1, batch_size, -1)\n",
    "    expanded = expanded + offsets\n",
    "    expanded = expanded.permute(1, 0, 2).reshape(2, -1)\n",
    "\n",
    "    expanded_values = values.repeat(batch_size)\n",
    "\n",
    "    size = (N * batch_size, N * batch_size)\n",
    "    return torch.sparse_coo_tensor(expanded, expanded_values, size=size, device=adj.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b772d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"model/model.pth\"\n",
    "trainer = Trainer(model_dir = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da168b22-5022-4d84-9f43-77fe037defd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.generate_comparison_gif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcb670-8829-4dee-862e-47a5372e1b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "serhans_venv",
   "language": "python",
   "name": "serhans_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
