{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4252f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from math import exp, sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9545a3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "from scipy.sparse import lil_matrix\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a622a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture_field(size, n_components=None):\n",
    "    x, y = np.meshgrid(np.linspace(0, 1, size), np.linspace(0, 1, size))\n",
    "    field = np.zeros((size, size))\n",
    "\n",
    "    if not n_components:\n",
    "        n_components = np.random.poisson(20)\n",
    "\n",
    "    for _ in range(n_components):\n",
    "        cx, cy = np.random.uniform(0, 1, 2) # random center\n",
    "        sx, sy = np.random.uniform(0.01, 0.2, 2) # random covariance scale\n",
    "        w = np.random.exponential(10) # weight scale\n",
    "\n",
    "        gaussian = w * np.exp(-(((x - cx) ** 2) / (2 * sx**2) +\n",
    "                                ((y - cy) ** 2) / (2 * sy**2)))\n",
    "        field += gaussian\n",
    "\n",
    "    field = (1 - (field - field.min()) / (field.max() - field.min()))*2\n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a0e3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator():\n",
    "    def __init__(self, size=256, wind_speed=0, wind_direction=[0,0], response_rate=0.1, response_start=20, base_spread_rate=0.3, n_components=None, decay_rate=1e-3):\n",
    "        self.size = size\n",
    "        self.map = np.zeros((size, size))\n",
    "        self.wind_speed = wind_speed\n",
    "        self.wind_direction = wind_direction\n",
    "        self.response_rate = response_rate\n",
    "        self.response_start = response_start\n",
    "        self.spread_rate = base_spread_rate\n",
    "        self.time = 0\n",
    "        self.decay_rate = decay_rate\n",
    "        self.maps = {}\n",
    "\n",
    "        self.terrain = gaussian_mixture_field(size, n_components=n_components)\n",
    "\n",
    "    def step(self):\n",
    "        new_map = deepcopy(self.map)\n",
    "\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if self.map[i, j] >= 1:\n",
    "                    if np.random.rand() < self.spread_rate*self.terrain[i, j]*np.exp(-self.decay_rate * self.time) and new_map[i, j] < 5:\n",
    "                        new_map[i, j] += 1\n",
    "                    \n",
    "                    for di in [-1, 0, 1]:\n",
    "                        for dj in [-1, 0, 1]:\n",
    "                            if di == 0 and dj == 0:\n",
    "                                continue\n",
    "\n",
    "                            ni, nj = i + di, j + dj\n",
    "                            spread_chance = self.spread_rate*self.map[i,j]\n",
    "\n",
    "                            if 0 <= ni < self.size and 0 <= nj < self.size:\n",
    "                                if self.wind_speed > 0:\n",
    "                                    wind_influence = (di * self.wind_direction[0] + dj * self.wind_direction[1]) / (np.linalg.norm(self.wind_direction) + 1e-6)\n",
    "                                    wind_influence *= np.random.normal(1, 0.5)\n",
    "\n",
    "\n",
    "                                    if wind_influence > 0:\n",
    "                                        spread_chance *= (1 + self.wind_speed * wind_influence)\n",
    "                                    spread_chance *= self.terrain[ni, nj]\n",
    "                                    spread_chance *= np.exp(-self.decay_rate * self.time)\n",
    "                                    spread_chance = np.clip(spread_chance, 0, 1)\n",
    "\n",
    "                                if np.random.rand() < spread_chance and new_map[ni, nj] <= new_map[i, j]:\n",
    "                                    if new_map[ni, nj] < 5:\n",
    "                                        if np.random.rand() <= exp(-self.time/1000):\n",
    "                                            new_map[ni, nj] += 1\n",
    "\n",
    "                                if self.time >= self.response_start and new_map[ni, nj] == 0:\n",
    "                                    if np.random.rand() < 1 - exp(-(self.response_rate*(0.5+self.terrain[i,j]) * (self.time - self.response_start))):\n",
    "                                        if new_map[i, j] > 0:\n",
    "                                            new_map[i, j] -= 1 # Firefighting effort\n",
    "                            if np.exp(-self.decay_rate * self.time) < 0.5:\n",
    "                                if ni < 0 or ni >= self.size or nj < 0 or nj >= self.size:\n",
    "                                    if np.random.rand() < 1 - exp(-(self.response_rate) * (self.time - self.response_start)):\n",
    "                                        if new_map[i, j] > 0:\n",
    "                                            new_map[i, j] -= 1 # Edge effect\n",
    "                    \n",
    "                else:\n",
    "                    if 1 < i < self.size - 1 and 1 < j < self.size - 1:\n",
    "                        neighbors_on_fire = np.sum(self.map[i-1:i+2, j-1:j+2] >= 1) - (1 if self.map[i, j] >= 1 else 0)\n",
    "                        if neighbors_on_fire >= 6 and new_map[i, j] == 0:\n",
    "                            new_map[i, j] += 1\n",
    "        \n",
    "        self.maps[self.time] = deepcopy(self.map)\n",
    "        self.map = new_map\n",
    "        self.time += 1\n",
    "\n",
    "    def simulate(self):\n",
    "        nodes = np.random.poisson(3)\n",
    "\n",
    "        x_init, y_init = np.random.randint(0, self.size, size=2)\n",
    "        self.map[x_init, y_init] = np.random.poisson(3)\n",
    "\n",
    "        for _ in range(nodes - 1):\n",
    "            while True:\n",
    "                x, y = np.random.randint(-20, 21, size=2)\n",
    "                if x_init+x < self.size and y_init+y < self.size:\n",
    "                    break\n",
    "                \n",
    "            if self.map[x_init+x, y_init+y] == 0:\n",
    "                self.map[x_init+x, y_init+y] = np.random.poisson(3)\n",
    "                break\n",
    "    \n",
    "        while np.any(self.map > 0):\n",
    "            self.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulator = Simulator(size=256, wind_speed=1.5, wind_direction=[1,2], response_rate=0.05, response_start=100, base_spread_rate=0.05)\n",
    "#simulator.simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d70e6035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heatmap_gif(simulator, filename=\"simulation.gif\", cmap=\"plasma\"):\n",
    "    times = sorted(simulator.maps.keys())\n",
    "    frames = [simulator.maps[t] for t in times]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # --- fire as background ---\n",
    "    vmin, vmax = np.min(frames), np.max(frames)\n",
    "    fire_img = ax.imshow(frames[0], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # --- terrain overlay ---\n",
    "    terrain_img = ax.imshow(simulator.terrain, cmap=\"Greens\", alpha=0.2)  # low alpha on top\n",
    "\n",
    "    cbar = fig.colorbar(fire_img, ax=ax)\n",
    "    cbar.set_label(\"Fire Intensity\", rotation=270, labelpad=15)\n",
    "\n",
    "    def update(frame):\n",
    "        fire_img.set_data(frame)    \n",
    "        return [fire_img, terrain_img]\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, update, frames=frames, interval=60, blit=True\n",
    "    )\n",
    "\n",
    "    ani.save(filename, writer=\"pillow\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# make_heatmap_gif(simulator, \"fire.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One approach could be using GNN's, which would result in a scalable network to different grid sizes\n",
    "# Another is to use a transformer, easier to train\n",
    "# I'm going to try a Graph Attention Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa9430ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_matrix(length, width):\n",
    "    N = length * width\n",
    "    adj = lil_matrix((N, N), dtype=np.float32)\n",
    "    directions = [(-1,0), (-1,1), (0,1), (1,1), (1,0), (1,-1), (0,-1), (-1,-1), (0,0)] # 8 sided\n",
    "    for i in range(N):\n",
    "        x, y = divmod(i, width)\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x+dx, y+dy\n",
    "            if 0 <= nx < length and 0 <= ny < width:\n",
    "                j = nx*width + ny\n",
    "                adj[i,j] = 1\n",
    "    return adj.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9f7d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireGraph(Dataset):\n",
    "    def __init__(self, length=256, width=256, path=\"simulation_data\"):\n",
    "        self.path = path\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "\n",
    "\n",
    "        adj = adjacency_matrix(self.length, self.width)\n",
    "        self.adjacency_matrix = torch.sparse_coo_tensor(\n",
    "            indices=torch.tensor(np.vstack((adj.row, adj.col)), dtype=torch.long),\n",
    "            values=torch.tensor(adj.data, dtype=torch.float32),\n",
    "            size=adj.shape\n",
    "        )\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "        self.save_dir = os.path.join(self.path, f\"{self.length}x{self.width}\")\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def generate_data(self, topology:np.array=None, past_info:np.array=None, wind_direction:np.array=np.array([0,0]),\n",
    "                         wind_speed:int=0, time:int=0, label:np.array=None):\n",
    "        \n",
    "        flat_topo = topology.ravel()\n",
    "        flat_info = past_info[:, :, 0].ravel()\n",
    "        flat_info_date = past_info[:, :, 1].ravel()\n",
    "        flat_label = label.ravel()\n",
    "        \n",
    "        data = np.stack([\n",
    "            flat_topo,\n",
    "            flat_info,\n",
    "            flat_info_date,\n",
    "            np.full(flat_topo.shape, wind_direction[0], dtype=np.float32),\n",
    "            np.full(flat_topo.shape, wind_direction[1], dtype=np.float32),\n",
    "            np.full(flat_topo.shape, wind_speed, dtype=np.float32),\n",
    "            np.full(flat_topo.shape, time, dtype=np.float32),\n",
    "            flat_label\n",
    "        ], axis=1)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def save_data(self, data:np.array):\n",
    "        np.save(os.path.join(self.save_dir, f\"{time.time():.0f}.npy\"), data)\n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        arrays = []\n",
    "        for file in os.listdir(self.save_dir):\n",
    "            if file.endswith(\".npy\"):\n",
    "                arr = np.load(os.path.join(self.save_dir, file))\n",
    "                arrays.append(arr)\n",
    "\n",
    "        if arrays:\n",
    "            self.data = np.concatenate(arrays, axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        arr = self.data[idx]  # arr shape = (N, F+1)\n",
    "        x = arr[:, :-1]       # (N, F)\n",
    "        y = arr[:, -1]        # (N,)\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(y).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "401dcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced https://github.com/gordicaleksa/pytorch-GAT/blob/main/The%20Annotated%20GAT%20(Cora).ipynb for some of the code.\n",
    "\n",
    "class BelieverModel(nn.Module):\n",
    "    def __init__(self, nodes=256*256, input_features=7, num_layers=3, num_heads=3, num_features_per_head=4, num_output_classes=5, dropout=False):\n",
    "        super().__init__()\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = False\n",
    "        self.N = nodes\n",
    "        self.num_heads = num_heads\n",
    "        self.num_features_per_head = num_features_per_head\n",
    "\n",
    "        self.initial_transformation = nn.Linear(input_features, num_heads * num_features_per_head, bias=False)\n",
    "        nn.init.xavier_normal_(self.initial_transformation.weight)\n",
    "\n",
    "        self.a_lefts = nn.ParameterList()\n",
    "        self.a_rights = nn.ParameterList()\n",
    "        self.Ws = nn.ParameterList()\n",
    "        for i in range(num_layers):\n",
    "            a_left = nn.Parameter(torch.zeros(size=(num_heads, num_features_per_head)))\n",
    "            nn.init.xavier_uniform_(a_left)\n",
    "            a_right = nn.Parameter(torch.zeros(size=(num_heads, num_features_per_head)))\n",
    "            nn.init.xavier_uniform_(a_right)\n",
    "            W = nn.Parameter(torch.zeros(size=(num_heads, num_features_per_head, num_features_per_head)))\n",
    "            nn.init.xavier_normal_(W)\n",
    "\n",
    "            self.a_lefts.append(a_left)\n",
    "            self.a_rights.append(a_right)\n",
    "            self.Ws.append(W)\n",
    "        \n",
    "        self.final_transformation = nn.Linear(num_heads*num_features_per_head, num_output_classes)\n",
    "        nn.init.xavier_normal_(self.final_transformation.weight)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # adj = (N, N) adjacency matrix, in coo matrix format\n",
    "        # x = inputs (N, F_in)\n",
    "\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        N = x.size(0)\n",
    "        x = self.initial_transformation(x) # (N, F_out*H)\n",
    "        x = x.view(N, self.num_heads, self.num_features_per_head) # (N, H, F_out)\n",
    "\n",
    "        for W, a_left, a_right in zip(self.Ws, self.a_lefts, self.a_rights):\n",
    "            # W = (H, F_out, F_out)\n",
    "            # a_left = (H, F_out)\n",
    "            # a_right = (H, F_out)\n",
    "\n",
    "            # alpha_i,j = exp(a * [h_i||h_j] * adj[i,j]) / sum_j(exp(a * [h_i||h_j] * adj[i,j])), softmax\n",
    "            # h(i') = adj * sum_j (alpha_i,j * W * h_j)\n",
    "            # to simplify, we split a into 2 parts a_left and a_right, and calculate the attention scores for each of those parts, then sum up the scores only for viable pairs for computational efficiency\n",
    "\n",
    "            h_prime = torch.einsum(\"nhf,hfo->nho\", x, W) # (N, H, F_out) x (H, F_out, F_out) -> (N, H, F_out)\n",
    "\n",
    "            source_scores = (h_prime * a_left).sum(-1) # elementwise product, (N, H)\n",
    "            neighbor_scores = (h_prime * a_right).sum(-1) # (N, H)\n",
    "\n",
    "            adj = adj.coalesce()\n",
    "            row, col = adj.indices()\n",
    "            row = row.long(); col = col.long()\n",
    "            e = self.leakyrelu(source_scores[row] + neighbor_scores[col]) # (E, H) where E is the number of edges\n",
    "\n",
    "            H =e.size(1)\n",
    "            if hasattr(torch.Tensor, \"scatter_reduce\"):\n",
    "                max_per_node = torch.zeros((N, H), device=e.device, dtype=e.dtype).scatter_reduce(\n",
    "                    0, row.unsqueeze(-1).expand(-1,H), e, reduce=\"amax\", include_self=False\n",
    "                ) # maximum score among all neighbors of node i, (N, H)\n",
    "            else:\n",
    "                # fallback: compute max per node manually (safe but slower)\n",
    "                max_per_node = torch.full((N,H), -1e9, device=e.device, dtype=e.dtype)\n",
    "                for i_edge, i_node in enumerate(row):\n",
    "                    max_per_node[i_node] = torch.maximum(max_per_node[i_node], e[i_edge])\n",
    "            exp_e = torch.exp(e - max_per_node[row]) # for numerical stability, (E, H)\n",
    "\n",
    "            denom = torch.zeros((N, H), device=e.device, dtype=e.dtype)\n",
    "            denom.index_add_(0, row, exp_e) # summation over neighbors, (N, H)\n",
    "\n",
    "            alpha = exp_e / (denom[row] + 1e-9) # elementwise division, (E, H)\n",
    "\n",
    "            messages = h_prime[col] * alpha.unsqueeze(-1) # (E, H, F)\n",
    "\n",
    "            out = torch.zeros_like(h_prime, device=h_prime.device, dtype=h_prime.dtype)\n",
    "            out.index_add_(0, row, messages) # summation over neighbors again, (N, H, F)\n",
    "            x = self.relu(out)\n",
    "        \n",
    "        x = x.reshape(N, self.num_heads * self.num_features_per_head)\n",
    "        logits = self.final_transformation(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator():\n",
    "    def __init__(self, size=256, wind_speed=1.5, wind_direction=[1,2], response_rate=0.05, response_start=100, base_spread_rate=0.05, perturb=True):\n",
    "        self.size = size\n",
    "        self.wind_speed = wind_speed\n",
    "        self.wind_direction = wind_direction\n",
    "        self.response_rate = response_rate\n",
    "        self.response_start = response_start\n",
    "        self.base_spread_rate = base_spread_rate\n",
    "        self.perturb = perturb\n",
    "        self.dataset = FireGraph(length=self.size, width=self.size)\n",
    "\n",
    "    def generate(self, num_sims=100, std_dev=0.2):\n",
    "        for i in range(num_sims):\n",
    "            if self.perturb:\n",
    "                \n",
    "                simulator = Simulator(size=self.size, wind_speed=np.random.normal(self.wind_speed, std_dev*self.wind_speed), wind_direction=[np.random.uniform(-1, 1), np.random.uniform(-1,1)], \n",
    "                    response_rate=max(np.random.normal(self.response_rate, std_dev*self.response_rate), 0.005),\n",
    "                    response_start=max(np.random.normal(self.response_start, math.floor(std_dev*self.response_start)), 0), \n",
    "                    base_spread_rate=max(np.random.normal(self.base_spread_rate, std_dev*self.base_spread_rate), 0),\n",
    "                )\n",
    "            else:\n",
    "                simulator = Simulator(size=self.size, wind_speed=self.wind_speed, wind_direction=self.wind_direction, response_rate=self.response_rate, \n",
    "                    response_start=self.response_start, base_spread_rate=self.base_spread_rate)\n",
    "                \n",
    "            simulator.simulate()\n",
    "\n",
    "            simulation_data = []\n",
    "            past_info = np.zeros((self.size, self.size, 2))\n",
    "            for t in simulator.maps:\n",
    "                if 0.5 > np.random.rand():\n",
    "                    # sample a local patch where an agent observed the previous map\n",
    "                    coords = np.random.randint(0, self.size, 2)   # FIX: use grid size, not hard-coded 256\n",
    "                    if t>0:\n",
    "                        prev_map = simulator.maps[t-1]\n",
    "                        y_min, y_max = max(0, coords[0]-3), min(self.size, coords[0]+4)\n",
    "                        x_min, x_max = max(0, coords[1]-3), min(self.size, coords[1]+4)\n",
    "\n",
    "                        past_info[y_min:y_max, x_min:x_max, 0] = prev_map[y_min:y_max, x_min:x_max]\n",
    "                        past_info[y_min:y_max, x_min:x_max, 1] = 0     \n",
    "\n",
    "                    past_info[:, :, 1] += 1\n",
    "\n",
    "                \n",
    "                data_point = self.dataset.generate_data(topology=simulator.terrain, past_info=past_info, wind_direction=np.array(simulator.wind_direction), \n",
    "                                           wind_speed=simulator.wind_speed, time=t, label=simulator.maps[t])\n",
    "                simulation_data.append(data_point)\n",
    "                \n",
    "            self.dataset.save_data(np.array(simulation_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97c7f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, size=256, model_layers=10, lr=1e-4, num_output_classes=5, model_dir=None):\n",
    "        self.generator = DatasetGenerator(size=size)\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        self.model = BelieverModel(num_layers=model_layers, num_output_classes=num_output_classes, nodes=size**2)\n",
    "        if model_dir:\n",
    "            self.model.load_state_dict(torch.load(model_dir))\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss() # ordinal labels\n",
    "\n",
    "    def generate(self, num_sims=100):\n",
    "        self.generator.generate(num_sims=num_sims)\n",
    "\n",
    "    def clear_data(self):\n",
    "        dir = self.generator.dataset.save_dir\n",
    "\n",
    "        for f in os.listdir(dir):\n",
    "            if f.endswith(\".npy\"):\n",
    "                os.remove(os.path.join(dir, f))\n",
    "\n",
    "    def train(self, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "              num_sims=100, num_epochs=100):\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            self.generate(num_sims=num_sims)\n",
    "\n",
    "            self.generator.dataset.generate_dataset()\n",
    "            print(f\"Generated the simulations for {epoch+1}/{num_epochs}.\")\n",
    "            train_loader = DataLoader(self.generator.dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_nodes = 0\n",
    "\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                B, N, F = batch_x.shape\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                flat_x = batch_x.reshape(B * N, F)\n",
    "                adj_batch = block_diag_batch(self.generator.dataset.adjacency_matrix.to(device), B)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model(flat_x, adj_batch)\n",
    "\n",
    "                loss = self.loss_fn(\n",
    "                    logits, \n",
    "                    batch_y.reshape(-1)\n",
    "                ) # averaged over all nodes in all maps\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                total_correct += (preds == batch_y).sum().item()\n",
    "                total_nodes += batch_y.numel()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            acc = total_correct / total_nodes\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "            self.clear_data()\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "        torch.save(self.model.state_dict(), self.model_dir)\n",
    "\n",
    "    def generate_comparison_gif(self, filename=\"comparison.gif\", cmap=\"plasma\", \n",
    "                                device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                                new_sim = False):\n",
    "\n",
    "        # Generate one simulation and load the saved .npy so we have the full time sequence\n",
    "        if new_sim:\n",
    "            self.generate(num_sims=1)\n",
    "\n",
    "        dataset = self.generator.dataset\n",
    "        # find the most recent saved simulation file in the dataset save_dir\n",
    "        files = [f for f in os.listdir(dataset.save_dir) if f.endswith('.npy')]\n",
    "        if not files:\n",
    "            raise RuntimeError(f\"No simulation files found in {dataset.save_dir}\")\n",
    "\n",
    "        latest = max(files, key=lambda f: os.path.getmtime(os.path.join(dataset.save_dir, f)))\n",
    "        sim_path = os.path.join(dataset.save_dir, latest)\n",
    "\n",
    "        sim = np.load(sim_path)  # shape = (T, N, F+1)\n",
    "        T = sim.shape[0]\n",
    "        N = sim.shape[1]\n",
    "        size = int(np.sqrt(N))\n",
    "\n",
    "        # terrain comes from the first timestep (static)\n",
    "        terrain = sim[0, :, 0].reshape(size, size)\n",
    "\n",
    "        # past_info and dates for every timestep\n",
    "        past_info_seq = sim[:, :, 1].reshape(T, size, size)\n",
    "        past_info_date_seq = sim[:, :, 2].reshape(T, size, size)\n",
    "\n",
    "        # truth_seq: (T, size, size)\n",
    "        truth_seq = sim[:, :, -1].reshape(T, size, size)\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Compute model predictions for each timestep (so predictions vary across frames)\n",
    "        pred_seq = []\n",
    "        with torch.no_grad():\n",
    "            for t in range(T):\n",
    "                x_t = torch.from_numpy(sim[t, :, :-1]).float().to(device)  # (N, F)\n",
    "                logits = self.model(x_t, dataset.adjacency_matrix.to(device))\n",
    "                pred_map = logits.argmax(dim=1).cpu().numpy().reshape(size, size)\n",
    "                pred_seq.append(pred_map)\n",
    "        pred_seq = np.array(pred_seq)\n",
    "\n",
    "        # compute vmin/vmax for fire panels (truth + model)\n",
    "        vmin_fire = float(min(truth_seq.min(), pred_seq.min()))\n",
    "        vmax_fire = float(max(truth_seq.max(), pred_seq.max()))\n",
    "\n",
    "        # compute vmin/vmax for knowledge panel\n",
    "        vmin_k = float(past_info_seq.min())\n",
    "        vmax_k = float(past_info_seq.max())\n",
    "\n",
    "        # pick a visualization scale to make knowledge more prominent relative to fire range\n",
    "        # scale knowledge so its max is roughly 80% of fire max (but at least 1.0)\n",
    "        if vmax_k <= 0:\n",
    "            knowledge_vis_scale = 1.0\n",
    "        else:\n",
    "            knowledge_vis_scale = max(1.0, (0.8 * (vmax_fire + 1e-9)) / (vmax_k))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        panels = [\"Truth\", \"Knowledge\", \"Model Prediction\"]\n",
    "        fire_imgs = []\n",
    "        terrain_imgs = []\n",
    "\n",
    "        for ax, title in zip(axes, panels):\n",
    "            ax.set_title(title)\n",
    "            ax.axis(\"off\")\n",
    "            # For Knowledge panel we'll create the image with its own vmin/vmax after\n",
    "            if title == \"Knowledge\":\n",
    "                fire_img = ax.imshow(np.zeros((size, size)), cmap=cmap, vmin=vmin_k * knowledge_vis_scale, vmax=vmax_k * knowledge_vis_scale)\n",
    "            else:\n",
    "                fire_img = ax.imshow(np.zeros((size, size)), cmap=cmap, vmin=vmin_fire, vmax=vmax_fire)\n",
    "            fire_imgs.append(fire_img)\n",
    "            # terrain overlay on top with low alpha, like make_heatmap_gif\n",
    "            terrain_img = ax.imshow(terrain, cmap=\"Greens\", alpha=0.2)\n",
    "            terrain_imgs.append(terrain_img)\n",
    "\n",
    "        # add a single colorbar for the fire panels (shared scale)\n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "        from matplotlib import colors\n",
    "        fig.colorbar(fire_imgs[0], cax=cbar_ax)\n",
    "\n",
    "        max_frame = truth_seq.shape[0]\n",
    "\n",
    "        def update(frame_idx):\n",
    "            # Truth for this timestep\n",
    "            fire_imgs[0].set_data(truth_seq[frame_idx])\n",
    "\n",
    "            # Knowledge, fading (use per-timestep stored past info) and scale for visibility\n",
    "            pinfo = past_info_seq[frame_idx]\n",
    "            pdate = past_info_date_seq[frame_idx]\n",
    "            tau = 10.0\n",
    "            fading_map = pinfo * np.exp(-pdate / tau)\n",
    "            fading_vis = fading_map * knowledge_vis_scale\n",
    "            fire_imgs[1].set_data(fading_vis)\n",
    "\n",
    "            # Model predictions for this timestep\n",
    "            fire_imgs[2].set_data(pred_seq[frame_idx])\n",
    "\n",
    "            return fire_imgs + terrain_imgs\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, update, frames=max_frame, interval=200, blit=False)\n",
    "        ani.save(filename, writer=\"pillow\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), self.model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42c8f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_diag_batch(adj, batch_size):\n",
    "    \"\"\"\n",
    "    Build block-diagonal adjacency matrix for efficient processing of batches.\n",
    "    \"\"\"\n",
    "    N = adj.size(0)\n",
    "    indices = adj.indices()\n",
    "    values = adj.values()\n",
    "\n",
    "    offsets = torch.arange(batch_size, device=indices.device) * N\n",
    "    offsets = offsets.view(1, -1, 1)\n",
    "\n",
    "    expanded = indices.unsqueeze(1).expand(-1, batch_size, -1)\n",
    "    expanded = expanded + offsets\n",
    "    expanded = expanded.permute(1, 0, 2).reshape(2, -1)\n",
    "\n",
    "    expanded_values = values.repeat(batch_size)\n",
    "\n",
    "    size = (N * batch_size, N * batch_size)\n",
    "    return torch.sparse_coo_tensor(expanded, expanded_values, size=size, device=adj.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b772d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/mreisigahrooei/mtaskin/serhans_venv/lib/python3.10/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"model/model.pth\"\n",
    "trainer = Trainer(model_dir = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bee068",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28c7137e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic percent-change check starting...\n",
      "Total timesteps (frames): 324\n",
      "Grid size: 64x64 (N=4096)\n",
      "\n",
      "past_info change per-timestep (fraction of cells changed compared to previous timestep):\n",
      "  min: 0.000000, max: 0.011963, mean: 0.000222, median: 0.000000\n",
      "  #frames with zero changes: 312 / 323 (96.59%)\n",
      "\n",
      "past_info_date change per-timestep (fraction of cells changed):\n",
      "  min: 0.000000, max: 1.000000, mean: 0.529412, median: 1.000000\n",
      "  #frames with zero date changes: 152 / 323 (47.06%)\n",
      "\n",
      "Frames where date changed but info did NOT change: 160\n",
      "  sample frame indices (relative to diff array, i.e. frame t compares t->t+1): [2, 4, 8, 10, 14, 15, 16, 18, 21, 24]\n",
      "  center cell past_info before/after: 0.0 -> 0.0\n",
      "  center cell past_info_date before/after: 0.0 -> 1.0\n",
      "\n",
      "Sample cells time-series summary (past_info, past_info_date):\n",
      "  cell (49,53): info_changes=0, date_changes=171, info_unique=[0.], date_unique_sample=[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  cell (5,33): info_changes=0, date_changes=171, info_unique=[0.], date_unique_sample=[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  cell (62,51): info_changes=0, date_changes=171, info_unique=[0.], date_unique_sample=[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  cell (38,61): info_changes=0, date_changes=171, info_unique=[0.], date_unique_sample=[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "  cell (45,27): info_changes=0, date_changes=171, info_unique=[0.], date_unique_sample=[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: percent-changes per timestep for past_info and past_info_date\n",
    "# Uses kernel variables: past_info_seq, past_info_date_seq (shapes: T x size x size)\n",
    "import numpy as np\n",
    "\n",
    "print('Diagnostic percent-change check starting...')\n",
    "\n",
    "if 'past_info_seq' not in globals() or 'past_info_date_seq' not in globals():\n",
    "    raise RuntimeError('past_info_seq or past_info_date_seq not found in kernel. Run the earlier cells to load a simulation first.')\n",
    "\n",
    "T, H, W = past_info_seq.shape\n",
    "N = H * W\n",
    "\n",
    "# compute per-timestep counts of changed cells (compared to previous timestep)\n",
    "info_changes_counts = ((past_info_seq[1:] != past_info_seq[:-1]).reshape(T-1, -1).sum(axis=1)).astype(int)\n",
    "date_changes_counts = ((past_info_date_seq[1:] != past_info_date_seq[:-1]).reshape(T-1, -1).sum(axis=1)).astype(int)\n",
    "\n",
    "info_pct = info_changes_counts / N\n",
    "date_pct = date_changes_counts / N\n",
    "\n",
    "from statistics import mean, median\n",
    "\n",
    "print(f'Total timesteps (frames): {T}')\n",
    "print(f'Grid size: {H}x{W} (N={N})')\n",
    "print('\\npast_info change per-timestep (fraction of cells changed compared to previous timestep):')\n",
    "print(f'  min: {info_pct.min():.6f}, max: {info_pct.max():.6f}, mean: {mean(info_pct):.6f}, median: {median(info_pct):.6f}')\n",
    "print(f'  #frames with zero changes: {(info_pct==0).sum()} / {T-1} ({(info_pct==0).sum()/(T-1)*100:.2f}%)')\n",
    "\n",
    "print('\\npast_info_date change per-timestep (fraction of cells changed):')\n",
    "print(f'  min: {date_pct.min():.6f}, max: {date_pct.max():.6f}, mean: {mean(date_pct):.6f}, median: {median(date_pct):.6f}')\n",
    "print(f'  #frames with zero date changes: {(date_pct==0).sum()} / {T-1} ({(date_pct==0).sum()/(T-1)*100:.2f}%)')\n",
    "\n",
    "# Frames where date changed but info did not\n",
    "frames_date_changed_only = np.where((date_pct > 0) & (info_pct == 0))[0]\n",
    "frames_info_changed = np.where(info_pct > 0)[0]\n",
    "frames_date_changed = np.where(date_pct > 0)[0]\n",
    "\n",
    "print(f'\\nFrames where date changed but info did NOT change: {len(frames_date_changed_only)}')\n",
    "if len(frames_date_changed_only) > 0:\n",
    "    sample = frames_date_changed_only[:10]\n",
    "    print('  sample frame indices (relative to diff array, i.e. frame t compares t->t+1):', sample.tolist())\n",
    "    # show a small patch comparison for the first such frame\n",
    "    fi = int(sample[0])\n",
    "    y,x = H//2, W//2\n",
    "    print('  center cell past_info before/after:', past_info_seq[fi, y, x], '->', past_info_seq[fi+1, y, x])\n",
    "    print('  center cell past_info_date before/after:', past_info_date_seq[fi, y, x], '->', past_info_date_seq[fi+1, y, x])\n",
    "\n",
    "# Quick inspect of unique values over time for a few random cells\n",
    "import random\n",
    "random.seed(0)\n",
    "cells = [(random.randint(0,H-1), random.randint(0,W-1)) for _ in range(5)]\n",
    "print('\\nSample cells time-series summary (past_info, past_info_date):')\n",
    "for (yi, xi) in cells:\n",
    "    vals_info = past_info_seq[:, yi, xi]\n",
    "    vals_date = past_info_date_seq[:, yi, xi]\n",
    "    n_changes_info = np.count_nonzero(vals_info[1:] != vals_info[:-1])\n",
    "    n_changes_date = np.count_nonzero(vals_date[1:] != vals_date[:-1])\n",
    "    print(f'  cell ({yi},{xi}): info_changes={n_changes_info}, date_changes={n_changes_date}, info_unique={np.unique(vals_info)}, date_unique_sample={np.unique(vals_date)[:10]}')\n",
    "\n",
    "print('\\nDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8fb057d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGenerator: source not available (source code not available)\n",
      "FireGraph: source not available (source code not available)\n",
      "--- DatasetGenerator.generate ---\n",
      "004:                 simulator = Simulator(size=self.size, wind_speed=np.random.normal(self.wind_speed, std_dev*self.wind_speed), wind_direction=[np.random.uniform(-1, 1), np.random.uniform(-1,1)], \n",
      "005:                     response_rate=max(np.random.normal(self.response_rate, std_dev*self.response_rate), 0.005),\n",
      "006:                     response_start=max(np.random.normal(self.response_start, math.floor(std_dev*self.response_start)), 0), \n",
      "007:                     base_spread_rate=max(np.random.normal(self.base_spread_rate, std_dev*self.base_spread_rate), 0),\n",
      "016:             past_info = np.zeros((self.size, self.size, 2))\n",
      "018:                 if 0.5 > np.random.rand():\n",
      "019:                     coords = np.random.randint(0, 256, 2)\n",
      "025:                         past_info[y_min:y_max, x_min:x_max, 0] = prev_map[y_min:y_max, x_min:x_max]\n",
      "026:                         past_info[y_min:y_max, x_min:x_max, 1] = 0     \n",
      "028:                     past_info[:, :, 1] += 1\n",
      "031:                 data_point = self.dataset.generate_data(topology=simulator.terrain, past_info=past_info, wind_direction=np.array(simulator.wind_direction), \n",
      "--- FireGraph.generate_data ---\n",
      "000:     def generate_data(self, topology:np.array=None, past_info:np.array=None, wind_direction:np.array=np.array([0,0]),\n",
      "004:         flat_info = past_info[:, :, 0].ravel()\n",
      "005:         flat_info_date = past_info[:, :, 1].ravel()\n"
     ]
    }
   ],
   "source": [
    "# Focused inspection: show only lines that mention past_info or past_info_date in DatasetGenerator and related generators\n",
    "import inspect\n",
    "\n",
    "def print_filtered(obj, name, keywords=('past_info','past_info_date','np.random','random','if 0.5','patch','sample')):\n",
    "    try:\n",
    "        src = inspect.getsource(obj)\n",
    "    except Exception as e:\n",
    "        print(f'{name}: source not available ({e})')\n",
    "        return\n",
    "    print(f'--- {name} ---')\n",
    "    for i, line in enumerate(src.splitlines()):\n",
    "        if any(k in line for k in keywords):\n",
    "            print(f'{i:03d}: {line}')\n",
    "\n",
    "for nm in ['DatasetGenerator', 'FireGraph', 'DatasetGenerator.generate', 'FireGraph.generate_data']:\n",
    "    try:\n",
    "        obj = eval(nm)\n",
    "    except Exception:\n",
    "        # try attribute lookup for method style names\n",
    "        if '.' in nm:\n",
    "            cls, m = nm.split('.')\n",
    "            try:\n",
    "                obj = getattr(eval(cls), m)\n",
    "            except Exception:\n",
    "                print(f'{nm}: not found')\n",
    "                continue\n",
    "        else:\n",
    "            print(f'{nm}: not found')\n",
    "            continue\n",
    "    print_filtered(obj, nm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf7a8a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regenerating a small simulation (size=64) with fixed DatasetGenerator...\n",
      "Loaded sim simulation_data/64x64/1758769571.npy shape (540, 4096, 8)\n",
      "\n",
      "After fix — past_info change per-timestep (fraction of cells changed):\n",
      "  min: 0.000000, max: 0.011963, mean: 0.000150, median: 0.000000\n",
      "  #frames with zero changes: 527 / 539 (97.77%)\n",
      "\n",
      "After fix — past_info_date change per-timestep (fraction of cells changed):\n",
      "  min: 0.000000, max: 1.000000, mean: 0.523191, median: 1.000000\n",
      "  #frames with zero date changes: 257 / 539 (47.68%)\n",
      "\n",
      "Frames where date changed but info did NOT change: 270\n",
      "Done — kept the generated .npy at simulation_data/64x64/1758769571.npy\n"
     ]
    }
   ],
   "source": [
    "# Re-generate a small simulation with fixed DatasetGenerator coords and re-run percent-change diagnostic\n",
    "print('Regenerating a small simulation (size=64) with fixed DatasetGenerator...')\n",
    "\n",
    "g = DatasetGenerator(size=64)\n",
    "# generate a single simulation (may take some time)\n",
    "g.generate(num_sims=1)\n",
    "\n",
    "# find latest npy in save_dir\n",
    "files = [f for f in os.listdir(g.dataset.save_dir) if f.endswith('.npy')]\n",
    "latest = max(files, key=lambda f: os.path.getmtime(os.path.join(g.dataset.save_dir, f)))\n",
    "sim_path = os.path.join(g.dataset.save_dir, latest)\n",
    "sim = np.load(sim_path)\n",
    "T = sim.shape[0]\n",
    "N = sim.shape[1]\n",
    "size = int(np.sqrt(N))\n",
    "print(f'Loaded sim {sim_path} shape {sim.shape}')\n",
    "\n",
    "past_info_seq = sim[:, :, 1].reshape(T, size, size)\n",
    "past_info_date_seq = sim[:, :, 2].reshape(T, size, size)\n",
    "\n",
    "# compute percent-change stats (same as earlier diagnostic)\n",
    "T, H, W = past_info_seq.shape\n",
    "N = H*W\n",
    "info_changes_counts = ((past_info_seq[1:] != past_info_seq[:-1]).reshape(T-1, -1).sum(axis=1)).astype(int)\n",
    "date_changes_counts = ((past_info_date_seq[1:] != past_info_date_seq[:-1]).reshape(T-1, -1).sum(axis=1)).astype(int)\n",
    "info_pct = info_changes_counts / N\n",
    "date_pct = date_changes_counts / N\n",
    "\n",
    "from statistics import mean, median\n",
    "print(f'\\nAfter fix — past_info change per-timestep (fraction of cells changed):')\n",
    "print(f'  min: {info_pct.min():.6f}, max: {info_pct.max():.6f}, mean: {mean(info_pct):.6f}, median: {median(info_pct):.6f}')\n",
    "print(f'  #frames with zero changes: {(info_pct==0).sum()} / {T-1} ({(info_pct==0).sum()/(T-1)*100:.2f}%)')\n",
    "print(f'\\nAfter fix — past_info_date change per-timestep (fraction of cells changed):')\n",
    "print(f'  min: {date_pct.min():.6f}, max: {date_pct.max():.6f}, mean: {mean(date_pct):.6f}, median: {median(date_pct):.6f}')\n",
    "print(f'  #frames with zero date changes: {(date_pct==0).sum()} / {T-1} ({(date_pct==0).sum()/(T-1)*100:.2f}%)')\n",
    "\n",
    "# show frames where date changed but info did not\n",
    "frames_date_changed_only = np.where((date_pct > 0) & (info_pct == 0))[0]\n",
    "print(f'\\nFrames where date changed but info did NOT change: {len(frames_date_changed_only)}')\n",
    "\n",
    "# keep the generated file for inspection (do not remove)\n",
    "print('Done — kept the generated .npy at', sim_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "serhans_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
